{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import exists\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.python.keras.models import Sequential, load_model\n",
    "from tensorflow.python.keras.layers import Dense, Conv1D, Dropout\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable GPU acceleration\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'wind_regression_model.h5'\n",
    "scaler_name = 'wind_regression_scaler'\n",
    "\n",
    "if not os.path.isdir(save_dir):\n",
    "    raise Exception(\"Model directory doesn't exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(scaler_path=None):\n",
    "    file = 'data/2020 velocity potential .995 sigma.csv'\n",
    "    df = pd.read_csv(file, sep=',')\n",
    "\n",
    "    x = df[['Lat','Lon']]\n",
    "    y = df[['Chi']]\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "    # open standard scaler if exists\n",
    "    if scaler_path is not None and exists(scaler_path):\n",
    "        # load wind model input scaler\n",
    "        yscaler = joblib.load(scaler_path)\n",
    "    else:   # else create new scaler\n",
    "        print(\"NO SCALER FOUND, BUILDING\")\n",
    "        yscaler = StandardScaler()\n",
    "        yscaler = yscaler.fit(y)\n",
    "    \n",
    "    # transform y_train and y_test to standardized scale\n",
    "    y_train = yscaler.transform(y_train)\n",
    "    y_test = yscaler.transform(y_test)\n",
    "    \n",
    "    # save if path given\n",
    "    if scaler_path is not None:\n",
    "        joblib.dump(yscaler, scaler_path)\n",
    "\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "\n",
    "    # normalise X input type\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, yscaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model design\n",
    "* This model design was experimentally defined to fit the dataset well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(32, input_dim=2, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x_train, y_train, x_test, y_test, epochs=150, batch_size=50, verbose=1, validation_split=0.2, save_dir=None, model_name=None):\n",
    "\n",
    "    history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose, validation_split=validation_split)\n",
    "\n",
    "    print(history.history.keys())\n",
    "\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # save model and weights\n",
    "    model_path = os.path.join(save_dir, model_name)\n",
    "    model.save(model_path)\n",
    "    print('Saved trained model at %s ' % model_path)\n",
    "    \n",
    "    # score trained model.\n",
    "    scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print('Test loss:', scores[0])\n",
    "    print('Test accuracy:', scores[1])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, yscaler, coords):\n",
    "    regression = model.predict(coords)\n",
    "    regression = yscaler.inverse_transform(regression)  # transform back to original scale\n",
    "\n",
    "    regression = pd.DataFrame(regression, columns=['Chi'])\n",
    "    regression = pd.concat([coords, regression], axis=1)\n",
    "    print(regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_path = os.path.join(save_dir, scaler_name)\n",
    "x_train, y_train, x_test, y_test, yscaler = load(scaler_path)\n",
    "\n",
    "model_path = os.path.join(save_dir, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exists(model_path):  # load model if exists\n",
    "    print(f\"LOADING MODEL: {model_path}\")\n",
    "\n",
    "    model = load_model(model_path)\n",
    "\n",
    "else:   # else generate new model\n",
    "    print(\"NO MODEL FOUND, BUILDING\")\n",
    "\n",
    "    model = build()\n",
    "\n",
    "    # train for large epoch size\n",
    "    model = train(\n",
    "        model, \n",
    "        x_train, \n",
    "        y_train, \n",
    "        x_test, \n",
    "        y_test, \n",
    "        epochs=200,         # test 1-5: 150, test 6: 200\n",
    "        batch_size=128,     # test 1-5: 50, test 6: 128\n",
    "        verbose=1, \n",
    "        validation_split=0.2,\n",
    "        save_dir=save_dir,\n",
    "        model_name=model_name\n",
    "    )\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_coords = pd.DataFrame([[-37.814, 144.96332], [-37.814, 144.96332]])\n",
    "test_coords.columns = ['Lat','Lon']\n",
    "\n",
    "test(\n",
    "    model,\n",
    "    yscaler,\n",
    "    coords=test_coords\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
